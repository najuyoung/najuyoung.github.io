---
title: (AI Boot Camp) CV and Competition info
date: 2025-06-16 12:00:01 +0900
categories: [ BOOTCAMP, KERNEL_ACADEMY ]
tags: [ 'AIBootCamp' ]
toc: true
comments: false
mermaid: true
math: true
---

# ğŸ“„ Document Type Classification ëŒ€íšŒ: ì‹¤ë¬´ ë¬¸ì„œ ë¶„ë¥˜ AI ëª¨ë¸ ê°œë°œ ê°€ì´ë“œ

## ğŸ“¦ ì‚¬ìš©í•˜ëŠ” python package

```python
# í•µì‹¬ ë¼ì´ë¸ŒëŸ¬ë¦¬
torch==2.1.0
torchvision==0.16.0
timm==0.9.10
albumentations
pandas
numpy
scikit-learn
PIL
tqdm

# ì‹¤í—˜ ê´€ë¦¬ (ì„ íƒì‚¬í•­)
wandb
```


## ğŸš€ TL;DR

- **Document Type Classification**ì€ 17ì¢…ì˜ ë¬¸ì„œ ì´ë¯¸ì§€ë¥¼ ìë™ìœ¼ë¡œ ë¶„ë¥˜í•˜ëŠ” Computer Vision ëŒ€íšŒë‹¤[^1_1]
- ê¸ˆìœµ, ì˜ë£Œ, ë³´í—˜ ë“± ì‹¤ì œ ì‚°ì—… í˜„ì¥ì—ì„œ í•„ìš”í•œ ë¬¸ì„œ ë””ì§€í„¸í™” ì‘ì—…ì„ AIë¡œ ìë™í™”í•˜ëŠ” ê²ƒì´ ëª©í‘œë‹¤[^1_1]
- ì´ 1,570ì¥ì˜ í•™ìŠµ ë°ì´í„°ì™€ 3,140ì¥ì˜ í…ŒìŠ¤íŠ¸ ë°ì´í„°ë¡œ êµ¬ì„±ë˜ë©°, í…ŒìŠ¤íŠ¸ ë°ì´í„°ì—ëŠ” í˜„ì‹¤ì ì¸ ë…¸ì´ì¦ˆê°€ í¬í•¨ë˜ì–´ ìˆë‹¤[^1_1]
- **ResNet34** ëª¨ë¸ê³¼ **Albumentations** ë¼ì´ë¸ŒëŸ¬ë¦¬ë¥¼ ì‚¬ìš©í•œ ë² ì´ìŠ¤ë¼ì¸ ì½”ë“œë¥¼ ì œê³µí•œë‹¤[^1_2]
- í‰ê°€ ì§€í‘œëŠ” **Macro F1 Score**ë¥¼ ì‚¬ìš©í•˜ì—¬ í´ë˜ìŠ¤ ë¶ˆê· í˜• ë¬¸ì œë¥¼ ê³ ë ¤í•œë‹¤[^1_1][^1_3]
- EDA, ë°ì´í„° ì¦ê°•, ì•™ìƒë¸” ë“± ë‹¤ì–‘í•œ ì„±ëŠ¥ í–¥ìƒ ê¸°ë²•ì„ ì ìš©í•  ìˆ˜ ìˆë‹¤[^1_1]


## ğŸ““ ì‹¤ìŠµ Jupyter Notebook

ì‹¤ìŠµ ì½”ë“œëŠ” ë‹¤ìŒ ê¹ƒí—ˆë¸Œ ë§í¬ì—ì„œ í™•ì¸í•  ìˆ˜ ìˆìŠµë‹ˆë‹¤:

- **ë² ì´ìŠ¤ë¼ì¸ ì½”ë“œ**: `Image-Classification-beiseurain-kodeu-haeseol.ipynb`
- **ëŒ€íšŒ ì†Œê°œ ìë£Œ**: `Image-Classification-daehoe-sogae.pdf`

---

## ğŸ¯ ëŒ€íšŒ ê°œìš”

### Document Type Classificationì´ë€?

**Document Type Classification**ì€ ë‹¤ì–‘í•œ ë¬¸ì„œ ì´ë¯¸ì§€ë¥¼ ì…ë ¥ë°›ì•„ í•´ë‹¹ ë¬¸ì„œì˜ íƒ€ì…ì„ ìë™ìœ¼ë¡œ ë¶„ë¥˜í•˜ëŠ” ì»´í“¨í„° ë¹„ì „ íƒœìŠ¤í¬ë‹¤[^1_4][^1_5]. ì´ëŠ” ì‹¤ì œ ì‚°ì—… í˜„ì¥ì—ì„œ ì•„ë‚ ë¡œê·¸ ë¬¸ì„œ ë°ì´í„°ì˜ ë””ì§€í„¸í™”ë¥¼ ìœ„í•´ í•„ìˆ˜ì ì¸ ê¸°ìˆ ì´ë‹¤[^1_6].

### ë¶„ë¥˜ ëŒ€ìƒ ë¬¸ì„œ íƒ€ì… (17ì¢…)

ì´ë²ˆ ëŒ€íšŒì—ì„œ ë¶„ë¥˜í•´ì•¼ í•˜ëŠ” ë¬¸ì„œ íƒ€ì…ì€ ë‹¤ìŒê³¼ ê°™ë‹¤[^1_1]:

- **ê¸ˆìœµ ê´€ë ¨**: ê³„ì¢Œë²ˆí˜¸, ì§„ë£Œë¹„ì˜ìˆ˜ì¦, ì•½ì œë¹„ ì˜ìˆ˜ì¦, ì§„ë£Œë¹„ ë‚©ì… í™•ì¸ì„œ
- **ì‹ ë¶„ì¦ëª…**: ì—¬ê¶Œ, ìš´ì „ë©´í—ˆì¦, ì£¼ë¯¼ë“±ë¡ì¦
- **ì°¨ëŸ‰ ê´€ë ¨**: ìë™ì°¨ ë²ˆí˜¸íŒ, ìë™ì°¨ ê³„ê¸°íŒ, ìë™ì°¨ ë“±ë¡ì¦
- **ì˜ë£Œ ê´€ë ¨**: ì²˜ë°©ì „, í†µì›/ì§„ë£Œ í™•ì¸ì„œ, ì…í‡´ì› í™•ì¸ì„œ, ì§„ë‹¨ì„œ, ì†Œê²¬ì„œ
- **ê¸°íƒ€**: ì´ë ¥ì„œ, ê±´ê°•ë³´í—˜ ì„ì‹ ì¶œì‚° ì§„ë£Œë¹„ ì§€ê¸‰ ì‹ ì²­ì„œ

> ì‹¤ì œ ì‚°ì—… í˜„ì¥ì—ì„œëŠ” ë¬¸ì„œ ë°ì´í„°ê°€ ê¸ˆìœµ, ì˜ë£Œ, ë³´í—˜, ë¬¼ë¥˜ ë“± ëª¨ë“  ë„ë©”ì¸ì— ì¡´ì¬í•˜ë©°, ë§ì€ íšŒì‚¬ë“¤ì´ ì•„ë‚ ë¡œê·¸ ë°ì´í„°ì˜ ë””ì§€í„¸í™”ë¥¼ í†µí•œ ë””ì§€í„¸ í˜ì‹ ì„ ì¶”ì§„í•˜ê³  ìˆë‹¤.
{: .prompt-tip}

## ğŸ“Š ë°ì´í„°ì…‹ êµ¬ì„±

### í•™ìŠµ ë°ì´í„°

- **ì´ 1,570ì¥**ì˜ ë¬¸ì„œ ì´ë¯¸ì§€
- **17ê°œ í´ë˜ìŠ¤**ë¡œ êµ¬ì„±
- ê° í´ë˜ìŠ¤ë³„ë¡œ **46~100ì¥**ì˜ ì´ë¯¸ì§€ í¬í•¨[^1_1]


### í…ŒìŠ¤íŠ¸ ë°ì´í„°

- **ì´ 3,140ì¥**ì˜ ë¬¸ì„œ ì´ë¯¸ì§€
- ì‹¤ì œ í˜„ì‹¤ ì„¸ê³„ì˜ ë…¸ì´ì¦ˆë¥¼ ë°˜ì˜í•œ ë‹¤ì–‘í•œ **augmentation**ì´ ì ìš©ë¨[^1_1]
- êµ¬ê²¨ì§„ ë¬¸ì„œ, ë¬¼ì— ì –ì€ ë¬¸ì„œ, ë¹›ë²ˆì§ ë“±ì˜ í˜„ì‹¤ì ì¸ ì™œê³¡ í¬í•¨[^1_1]


### ë°ì´í„° êµ¬ì¡°

```
datasets_fin/
â”œâ”€â”€ train.csv          # í•™ìŠµ ì´ë¯¸ì§€ ì´ë¦„ê³¼ í´ë˜ìŠ¤ ë§¤í•‘
â”œâ”€â”€ meta.csv           # í´ë˜ìŠ¤ ì´ë¦„ê³¼ ì¸ë±ìŠ¤ ë§¤í•‘  
â”œâ”€â”€ sample_submission.csv  # ì œì¶œìš© í…œí”Œë¦¿
â”œâ”€â”€ train/             # í•™ìŠµ ì´ë¯¸ì§€ í´ë”
â””â”€â”€ test/              # í…ŒìŠ¤íŠ¸ ì´ë¯¸ì§€ í´ë”
```


## ğŸ“ˆ í‰ê°€ ì§€í‘œ: Macro F1 Score

### F1 Scoreì˜ ê°œë…

**F1 Score**ëŠ” ì •ë°€ë„(Precision)ì™€ ì¬í˜„ìœ¨(Recall)ì˜ ì¡°í™”í‰ê· ìœ¼ë¡œ ê³„ì‚°ë˜ëŠ” ë¶„ë¥˜ ì„±ëŠ¥ ì§€í‘œë‹¤[^1_7][^1_8]. í´ë˜ìŠ¤ ë¶ˆê· í˜• ë¬¸ì œê°€ ìˆì„ ë•Œ ëª¨ë¸ì˜ ì„±ëŠ¥ì„ ì •í™•í•˜ê²Œ í‰ê°€í•  ìˆ˜ ìˆë‹¤[^1_9].

$F1 = 2 \times \frac{Precision \times Recall}{Precision + Recall}$

### Macro F1 Score ê³„ì‚° ë°©ë²•

**Macro F1 Score**ëŠ” ê° í´ë˜ìŠ¤ë³„ë¡œ ê°œë³„ì ìœ¼ë¡œ ê³„ì‚°ëœ F1 Scoreì˜ ë‹¨ìˆœ í‰ê· ì´ë‹¤[^1_3][^1_7]. ì´ëŠ” í´ë˜ìŠ¤ ë¹ˆë„ì— ê´€ê³„ì—†ì´ ëª¨ë“  í´ë˜ìŠ¤ë¥¼ ë™ë“±í•˜ê²Œ ì·¨ê¸‰í•œë‹¤[^1_8].

```python
# Macro F1 Score ê³„ì‚° ì˜ˆì‹œ
from sklearn.metrics import f1_score

def macro_f1_score(y_true, y_pred, n_classes):
    f1_scores = []
    
    for c in range(n_classes):
        y_true_c = (y_true == c)
        y_pred_c = (y_pred == c)
        f1_c = f1_score(y_true_c, y_pred_c)
        f1_scores.append(f1_c)
    
    return np.mean(f1_scores)
```


### Confusion Matrix ì´í•´

**Confusion Matrix**ëŠ” ì‹¤ì œ í´ë˜ìŠ¤ì™€ ì˜ˆì¸¡ í´ë˜ìŠ¤ë¥¼ ë¹„êµí•˜ì—¬ ëª¨ë¸ì˜ ì„±ëŠ¥ì„ ì‹œê°í™”í•˜ëŠ” ë„êµ¬ë‹¤[^1_10]. ë‹¤ìŒ ë„¤ ê°€ì§€ ìš”ì†Œë¡œ êµ¬ì„±ëœë‹¤[^1_10]:

- **TP (True Positive)**: ì‹¤ì œ positiveë¥¼ positiveë¡œ ì˜ˆì¸¡ (ì •ë‹µ)
- **FP (False Positive)**: ì‹¤ì œ negativeë¥¼ positiveë¡œ ì˜ˆì¸¡ (ì˜¤ë‹µ)
- **FN (False Negative)**: ì‹¤ì œ positiveë¥¼ negativeë¡œ ì˜ˆì¸¡ (ì˜¤ë‹µ)
- **TN (True Negative)**: ì‹¤ì œ negativeë¥¼ negativeë¡œ ì˜ˆì¸¡ (ì •ë‹µ)


## ğŸ”§ ë² ì´ìŠ¤ë¼ì¸ ëª¨ë¸ êµ¬í˜„

### ëª¨ë¸ ì•„í‚¤í…ì²˜: ResNet34

**ResNet (Residual Network)**ì€ ê¹Šì€ ì‹ ê²½ë§ì—ì„œ ë°œìƒí•˜ëŠ” ê¸°ìš¸ê¸° ì†Œì‹¤ ë¬¸ì œë¥¼ í•´ê²°í•˜ê¸° ìœ„í•´ ê°œë°œëœ CNN ì•„í‚¤í…ì²˜ë‹¤[^1_11]. **Skip Connection**ì„ í†µí•´ ê¸°ìš¸ê¸°ê°€ ì‰½ê²Œ ì „íŒŒë  ìˆ˜ ìˆë„ë¡ ì„¤ê³„ë˜ì—ˆë‹¤[^1_11].

```python
import timm
import torch.nn as nn

# ResNet34 ëª¨ë¸ ë¡œë“œ
model = timm.create_model(
    'resnet34',
    pretrained=True,
    num_classes=17  # 17ê°œ ë¬¸ì„œ í´ë˜ìŠ¤
).to(device)

loss_fn = nn.CrossEntropyLoss()
optimizer = Adam(model.parameters(), lr=1e-3)
```


### ë°ì´í„° ì „ì²˜ë¦¬ ë° ì¦ê°•

**Albumentations** ë¼ì´ë¸ŒëŸ¬ë¦¬ë¥¼ ì‚¬ìš©í•˜ì—¬ ì´ë¯¸ì§€ ì „ì²˜ë¦¬ì™€ ë°ì´í„° ì¦ê°•ì„ ìˆ˜í–‰í•œë‹¤[^1_12]. ì´ëŠ” 70ê°œ ì´ìƒì˜ ë‹¤ì–‘í•œ ì¦ê°• ê¸°ë²•ì„ ì œê³µí•˜ë©° PyTorchì™€ ì™„ë²½í•˜ê²Œ í˜¸í™˜ëœë‹¤[^1_12].

```python
import albumentations as A
from albumentations.pytorch import ToTensorV2

# í•™ìŠµìš© ë³€í™˜
trn_transform = A.Compose([
    A.Resize(height=224, width=224),
    A.Normalize(mean=[0.485, 0.456, 0.406], 
                std=[0.229, 0.224, 0.225]),
    ToTensorV2(),
])

# í…ŒìŠ¤íŠ¸ìš© ë³€í™˜
tst_transform = A.Compose([
    A.Resize(height=224, width=224),
    A.Normalize(mean=[0.485, 0.456, 0.406], 
                std=[0.229, 0.224, 0.225]),
    ToTensorV2(),
])
```


### ë°ì´í„°ì…‹ í´ë˜ìŠ¤ êµ¬í˜„

```python
from torch.utils.data import Dataset
import pandas as pd
from PIL import Image
import numpy as np

class ImageDataset(Dataset):
    def __init__(self, csv, path, transform=None):
        self.df = pd.read_csv(csv).values
        self.path = path
        self.transform = transform
    
    def __len__(self):
        return len(self.df)
    
    def __getitem__(self, idx):
        name, target = self.df[idx]
        img = np.array(Image.open(os.path.join(self.path, name)))
        
        if self.transform:
            img = self.transform(image=img)['image']
        
        return img, target
```


## ğŸš€ ëª¨ë¸ í•™ìŠµ ë° ì¶”ë¡ 

### í•™ìŠµ ë£¨í”„ êµ¬í˜„

```python
def train_one_epoch(loader, model, optimizer, loss_fn, device):
    model.train()
    train_loss = 0
    preds_list = []
    targets_list = []
    
    pbar = tqdm(loader)
    for image, targets in pbar:
        image = image.to(device)
        targets = targets.to(device)
        
        model.zero_grad(set_to_none=True)
        
        preds = model(image)
        loss = loss_fn(preds, targets)
        loss.backward()
        optimizer.step()
        
        train_loss += loss.item()
        preds_list.extend(preds.argmax(dim=1).detach().cpu().numpy())
        targets_list.extend(targets.detach().cpu().numpy())
        
        pbar.set_description(f"Loss: {loss.item():.4f}")
    
    # ì„±ëŠ¥ ì§€í‘œ ê³„ì‚°
    train_loss /= len(loader)
    train_acc = accuracy_score(targets_list, preds_list)
    train_f1 = f1_score(targets_list, preds_list, average='macro')
    
    return {
        "train_loss": train_loss,
        "train_acc": train_acc,
        "train_f1": train_f1,
    }
```


### ì¶”ë¡  ë° ê²°ê³¼ ì €ì¥

```python
# ëª¨ë¸ì„ í‰ê°€ ëª¨ë“œë¡œ ì„¤ì •
model.eval()
preds_list = []

for image, _ in tqdm(tst_loader):
    image = image.to(device)
    
    with torch.no_grad():
        preds = model(image)
        preds_list.extend(preds.argmax(dim=1).detach().cpu().numpy())

# ê²°ê³¼ ì €ì¥
pred_df = pd.DataFrame(tst_dataset.df, columns=['ID', 'target'])
pred_df['target'] = preds_list
pred_df.to_csv("submission.csv", index=False)
```


## ğŸ¨ ì„±ëŠ¥ í–¥ìƒ ê¸°ë²•

### EDA (Exploratory Data Analysis)

**íƒìƒ‰ì  ë°ì´í„° ë¶„ì„**ì„ í†µí•´ ë°ì´í„°ì˜ íŠ¹ì„±ì„ íŒŒì•…í•˜ê³  ëª¨ë¸ ê°œì„  ë°©í–¥ì„ ì„¤ì •í•  ìˆ˜ ìˆë‹¤[^1_1]:

- **ì´ë¯¸ì§€ ì‹œê°í™”**: í•™ìŠµ ë°ì´í„°ëŠ” cleaní•˜ê³  í…ŒìŠ¤íŠ¸ ë°ì´í„°ëŠ” noisyí•œ íŠ¹ì„± íŒŒì•…
- **íšŒì „ ë¬¸ì œ**: í…ŒìŠ¤íŠ¸ ì´ë¯¸ì§€ì— íšŒì „ëœ ê²½ìš°ê°€ ë§ì•„ rotation augmentation í•„ìš”
- **ì´ì§ˆì  ì´ë¯¸ì§€**: ì°¨ëŸ‰ ê´€ë ¨ ì´ë¯¸ì§€ë“¤ì´ ë‹¤ë¥¸ ë¬¸ì„œë“¤ê³¼ ìƒì´í•œ íŠ¹ì„±ì„ ë³´ì„
- **í¬ê¸° ë¶„í¬**: ë¬¸ì„œ ì´ë¯¸ì§€ì˜ í¬ê¸° ë¶„í¬ë¥¼ íŒŒì•…í•˜ì—¬ ì ì ˆí•œ resize ì „ëµ ìˆ˜ë¦½


### ë°ì´í„° ì¦ê°• ê¸°ë²•

**Document-specific augmentation**ì„ ìœ„í•´ **Augraphy** ë¼ì´ë¸ŒëŸ¬ë¦¬ í™œìš©ì„ ê³ ë ¤í•  ìˆ˜ ìˆë‹¤[^1_1]. ì´ëŠ” ë¬¸ì„œ ì´ë¯¸ì§€ì— íŠ¹í™”ëœ ë‹¤ì–‘í•œ ì¦ê°• ê¸°ë²•ì„ ì œê³µí•œë‹¤[^1_1]:

- **BleedThrough**: ì¢…ì´ ë’·ë©´ ë‚´ìš©ì´ ë¹„ì¹˜ëŠ” íš¨ê³¼
- **BadPhotoCopy**: ë³µì‚¬ê¸° í’ˆì§ˆ ì €í•˜ íš¨ê³¼
- **BookBinding**: ì±… ì œë³¸ìœ¼ë¡œ ì¸í•œ ì™œê³¡
- **ColorShift**: ìƒ‰ìƒ ë³€í™” íš¨ê³¼
- **InkMottling**: ì‰í¬ ë²ˆì§ íš¨ê³¼


### ì•™ìƒë¸” ê¸°ë²•

**Ensemble Methods**ëŠ” ì—¬ëŸ¬ ëª¨ë¸ì˜ ì˜ˆì¸¡ì„ ê²°í•©í•˜ì—¬ ë‹¨ì¼ ëª¨ë¸ë³´ë‹¤ ë” ë‚˜ì€ ì„±ëŠ¥ì„ ë‹¬ì„±í•˜ëŠ” ê¸°ë²•ì´ë‹¤[^1_13]. ë‹¤ìŒê³¼ ê°™ì€ ë°©ë²•ë“¤ì„ ì ìš©í•  ìˆ˜ ìˆë‹¤[^1_1]:

- **Model Ensemble**: ë‹¤ì–‘í•œ ì•„í‚¤í…ì²˜ ì¡°í•© (ResNet, EfficientNet, ViT)
- **Data Ensemble**: ë‹¤ì–‘í•œ ì´ë¯¸ì§€ í¬ê¸° ë° ì¦ê°• ê¸°ë²• ì ìš©
- **Seed Ensemble**: ë™ì¼í•œ ëª¨ë¸ì„ ë‹¤ë¥¸ random seedë¡œ í•™ìŠµ
- **Soft Voting**: ê° ëª¨ë¸ì˜ í™•ë¥ ê°’ì„ í‰ê· í•˜ì—¬ ìµœì¢… ì˜ˆì¸¡

```python
# ì•™ìƒë¸” ì˜ˆì‹œ
def ensemble_predict(models, data_loader, device):
    all_preds = []
    
    for model in models:
        model.eval()
        preds = []
        
        with torch.no_grad():
            for images, _ in data_loader:
                images = images.to(device)
                outputs = model(images)
                preds.extend(torch.softmax(outputs, dim=1).cpu().numpy())
        
        all_preds.append(np.array(preds))
    
    # Soft voting
    ensemble_preds = np.mean(all_preds, axis=0)
    final_preds = np.argmax(ensemble_preds, axis=1)
    
    return final_preds
```


## ğŸ”¬ ì‹¤í—˜ ê´€ë¦¬: Weights \& Biases

### W\&B í™œìš©ë²•

**Weights \& Biases**ëŠ” ë¨¸ì‹ ëŸ¬ë‹ ì‹¤í—˜ì„ ì²´ê³„ì ìœ¼ë¡œ ê´€ë¦¬í•˜ê³  ì‹œê°í™”í•˜ëŠ” í”Œë«í¼ì´ë‹¤[^1_14]. ë‹¤ìŒê³¼ ê°™ì€ ê¸°ëŠ¥ì„ ì œê³µí•œë‹¤[^1_14]:

- **ì‹¤í—˜ ì¶”ì **: ì†ì‹¤ê°’, ì •í™•ë„, í•˜ì´í¼íŒŒë¼ë¯¸í„° ìë™ ê¸°ë¡
- **ì‹¤ì‹œê°„ ëª¨ë‹ˆí„°ë§**: í•™ìŠµ ê³¼ì •ì„ ì‹¤ì‹œê°„ìœ¼ë¡œ ì‹œê°í™”
- **ë¹„êµ ë¶„ì„**: ì—¬ëŸ¬ ì‹¤í—˜ ê²°ê³¼ë¥¼ í•œëˆˆì— ë¹„êµ
- **í˜‘ì—… ë„êµ¬**: íŒ€ì›ë“¤ê³¼ ì‹¤í—˜ ê²°ê³¼ ê³µìœ 

```python
import wandb

# W&B ì´ˆê¸°í™”
wandb.init(
    project="document-classification",
    config={
        "learning_rate": 1e-3,
        "batch_size": 32,
        "epochs": 10,
        "model": "resnet34"
    }
)

# í•™ìŠµ ì¤‘ ë©”íŠ¸ë¦­ ë¡œê¹…
for epoch in range(epochs):
    metrics = train_one_epoch(train_loader, model, optimizer, loss_fn, device)
    
    wandb.log({
        "epoch": epoch,
        "train_loss": metrics["train_loss"],
        "train_acc": metrics["train_acc"],
        "train_f1": metrics["train_f1"]
    })
```


## ğŸ¯ ìµœì í™” ì „ëµ

### í•˜ì´í¼íŒŒë¼ë¯¸í„° íŠœë‹

- **Learning Rate**: Cosine Annealing ìŠ¤ì¼€ì¤„ëŸ¬ ì‚¬ìš©
- **Batch Size**: GPU ë©”ëª¨ë¦¬ì— ë”°ë¼ ì¡°ì •
- **Image Size**: 224x224ì—ì„œ ì‹œì‘í•˜ì—¬ ì ì§„ì ìœ¼ë¡œ ì¦ê°€
- **Augmentation**: í…ŒìŠ¤íŠ¸ ë°ì´í„° íŠ¹ì„±ì— ë§ëŠ” ì¦ê°• ê¸°ë²• ì„ íƒ


### ëª¨ë¸ ì„ íƒ ê¸°ì¤€

- **ê²½ëŸ‰ ëª¨ë¸**: MobileNetV2, EfficientNet-B0
- **ì¤‘ê°„ ì„±ëŠ¥**: ResNet50, EfficientNet-B3
- **ê³ ì„±ëŠ¥ ëª¨ë¸**: EfficientNet-B7, Vision Transformer

> ë¬¸ì„œ ë¶„ë¥˜ íƒœìŠ¤í¬ì˜ íŠ¹ì„±ìƒ ì •í™•í•œ í…ìŠ¤íŠ¸ ì •ë³´ ì¸ì‹ë³´ë‹¤ëŠ” ë¬¸ì„œì˜ ì „ì²´ì ì¸ ë ˆì´ì•„ì›ƒê³¼ êµ¬ì¡°ë¥¼ íŒŒì•…í•˜ëŠ” ê²ƒì´ ì¤‘ìš”í•˜ë‹¤. ë”°ë¼ì„œ ì ì ˆí•œ receptive fieldë¥¼ ê°€ì§„ CNN ëª¨ë¸ì´ íš¨ê³¼ì ì¼ ìˆ˜ ìˆë‹¤.
{: .prompt-tip}

## ğŸ† ìµœì¢… ì œì¶œ ë° ê²€ì¦

### êµì°¨ ê²€ì¦

```python
from sklearn.model_selection import StratifiedKFold

# 5-Fold êµì°¨ ê²€ì¦
skf = StratifiedKFold(n_splits=5, shuffle=True, random_state=42)

for fold, (train_idx, val_idx) in enumerate(skf.split(X, y)):
    print(f"Training Fold {fold+1}")
    
    # í´ë“œë³„ í•™ìŠµ ë° ê²€ì¦
    train_dataset = Subset(full_dataset, train_idx)
    val_dataset = Subset(full_dataset, val_idx)
    
    # ëª¨ë¸ í•™ìŠµ ë° í‰ê°€
    model = train_model(train_dataset, val_dataset)
    
    # ê²€ì¦ ì ìˆ˜ ê¸°ë¡
    val_score = evaluate_model(model, val_dataset)
    print(f"Fold {fold+1} Validation F1: {val_score:.4f}")
```


### ìµœì¢… ì œì¶œ íŒŒì¼ ìƒì„±

```python
# ìµœì¢… ì˜ˆì¸¡ ê²°ê³¼ ìƒì„±
final_predictions = ensemble_predict(best_models, test_loader, device)

# ì œì¶œ íŒŒì¼ ìƒì„±
submission_df = pd.read_csv("sample_submission.csv")
submission_df['target'] = final_predictions
submission_df.to_csv("final_submission.csv", index=False)

print("ì œì¶œ íŒŒì¼ì´ ìƒì„±ë˜ì—ˆìŠµë‹ˆë‹¤!")
print(f"ì˜ˆì¸¡ ê²°ê³¼ ë¶„í¬:\n{pd.Series(final_predictions).value_counts()}")
```


---

## ğŸ’¡ í•µì‹¬ ì¸ì‚¬ì´íŠ¸

**Document Type Classification**ì€ ë‹¨ìˆœí•œ ì´ë¯¸ì§€ ë¶„ë¥˜ë¥¼ ë„˜ì–´ ì‹¤ì œ ì‚°ì—… í˜„ì¥ì˜ ë¬¸ì œë¥¼ í•´ê²°í•˜ëŠ” ì‹¤ìš©ì ì¸ AI ê¸°ìˆ ì´ë‹¤[^1_6]. ì„±ê³µì ì¸ ëª¨ë¸ ê°œë°œì„ ìœ„í•´ì„œëŠ” ë‹¤ìŒê³¼ ê°™ì€ ìš”ì†Œë“¤ì´ ì¤‘ìš”í•˜ë‹¤:

- **ë„ë©”ì¸ íŠ¹í™” ì „ì²˜ë¦¬**: ë¬¸ì„œ ì´ë¯¸ì§€ì˜ íŠ¹ì„±ì„ ê³ ë ¤í•œ ì ì ˆí•œ ì „ì²˜ë¦¬ ë° ì¦ê°•
- **í´ë˜ìŠ¤ ë¶ˆê· í˜• ëŒ€ì‘**: Macro F1 Scoreë¥¼ ê³ ë ¤í•œ í•™ìŠµ ì „ëµ ìˆ˜ë¦½
- **í˜„ì‹¤ì  ë…¸ì´ì¦ˆ ì²˜ë¦¬**: í…ŒìŠ¤íŠ¸ ë°ì´í„°ì˜ ë…¸ì´ì¦ˆì— ê°•ê±´í•œ ëª¨ë¸ ì„¤ê³„
- **ì²´ê³„ì  ì‹¤í—˜ ê´€ë¦¬**: W\&B ë“±ì„ í™œìš©í•œ íš¨ìœ¨ì ì¸ ì‹¤í—˜ ì¶”ì 

ì´ëŸ¬í•œ ì ‘ê·¼ ë°©ì‹ì„ í†µí•´ ê¸ˆìœµ, ì˜ë£Œ, ë³´í—˜ ë“± ë‹¤ì–‘í•œ ì‚°ì—… ë¶„ì•¼ì—ì„œ í™œìš© ê°€ëŠ¥í•œ ì‹¤ìš©ì ì¸ ë¬¸ì„œ ë¶„ë¥˜ ì‹œìŠ¤í…œì„ êµ¬ì¶•í•  ìˆ˜ ìˆë‹¤.

<div style="text-align: center">â‚</div>

[^1_1]: Image-Classification-daehoe-sogae.pdf

[^1_2]: Image-Classification-beiseurain-kodeu-haeseol.ipynb

[^1_3]: https://data-minggeul.tistory.com/11

[^1_4]: https://paperswithcode.com/task/document-image-classification

[^1_5]: https://www.docsumo.com/blogs/ocr/document-classification

[^1_6]: https://www.linkedin.com/pulse/next-generation-document-classification-exploring-vision-srinivas-rqftc

[^1_7]: https://velog.io/@e1kim/ë¶„ë¥˜í‰ê°€ì§€í‘œ-Precision-Recall-F1-Macro-Micro-score

[^1_8]: https://velog.io/@nata0919/ë¶„ë¥˜-ì„±ëŠ¥-í‰ê°€-ì§€í‘œ-F1-Score-F-Beta-Score-Macro-F1-ì •ë¦¬

[^1_9]: https://dacon.io/en/forum/408130

[^1_10]: https://datasciencedojo.com/blog/confusion-matrix/

[^1_11]: https://cs231n.stanford.edu/reports/2017/pdfs/12.pdf

[^1_12]: https://github.com/De30/albumentations

[^1_13]: https://www.ultralytics.com/glossary/ensemble

[^1_14]: https://www.appvizer.com/artificial-intelligence/monitoringofexperiments/weights-biases

[^1_15]: https://www.altexsoft.com/blog/document-classification/

[^1_16]: https://wikidocs.net/22891

[^1_17]: https://blog.csdn.net/gitblog_00060/article/details/141013644

[^1_18]: https://www.mathworks.com/help/images/get-started-with-image-preprocessing-and-augmentation-for-deep-learning.html

[^1_19]: https://www.youtube.com/watch?v=OP8AozaEuLM

[^1_20]: https://affine.ai/learn-how-to-classify-documents-using-computer-vision-and-nlp/

[^1_21]: https://faiiry9.tistory.com/92

[^1_22]: https://scikit-learn.org/stable/modules/generated/sklearn.metrics.f1_score.html

